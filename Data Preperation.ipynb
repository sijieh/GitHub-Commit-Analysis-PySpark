{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "## Reading Git Final Project"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "import os\nimport subprocess\nimport datetime\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', None)"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": "gcs_folder = 'gs://msca-bdp-data-open/final_project_git'"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Check data size in GCS"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Total directory size: 1.36 TiB     gs://msca-bdp-data-open/final_project_git\n\n"}], "source": "cmd = 'gsutil du -s -h ' + gcs_folder\n\np = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\nfor line in p.stdout.readlines():\n    print (f'Total directory size: {line}')\n    \nretval = p.wait() # Wait for the child process to terminate."}, {"cell_type": "markdown", "metadata": {}, "source": "### Read Git data from GCS"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Languages\nProgramming languages by repository as reported by GitHub's https://developer.github.com/v3/repos/#list-languages API"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 1:=============================>                             (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "Records read from dataframe *languages*: 3,325,634\nCPU times: user 5.64 ms, sys: 10.5 ms, total: 16.1 ms\nWall time: 8.6 s\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "%%time   \n    \ndf_languages = spark.read.parquet(os.path.join(gcs_folder, 'languages'))\nprint(f'Records read from dataframe *languages*: {df_languages.count():,.0f}')"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- repo_name: string (nullable = true)\n |-- language: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- name: string (nullable = true)\n |    |    |-- bytes: long (nullable = true)\n\n"}], "source": "df_languages.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Licenses\nOpen source license SPDX code for each repository as detected by https://developer.github.com/v3/licenses/"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Records read from dataframe *licenses*: 3,325,634\nCPU times: user 4.85 ms, sys: 169 \u00b5s, total: 5.02 ms\nWall time: 901 ms\n"}], "source": "%%time   \n    \ndf_licenses = spark.read.parquet(os.path.join(gcs_folder, 'licenses'))\nprint(f'Records read from dataframe *licenses*: {df_licenses.count():,.0f}')"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- repo_name: string (nullable = true)\n |-- license: string (nullable = true)\n\n"}], "source": "df_licenses.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Commits\nUnique Git commits from open source repositories on GitHub, pre-grouped by repositories they appear in."}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 9:====================================================>(3483 + 1) / 3484]\r"}, {"name": "stdout", "output_type": "stream", "text": "Records read from dataframe *commits*: 265,419,190\nCPU times: user 1.08 s, sys: 257 ms, total: 1.34 s\nWall time: 4min 36s\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "%%time   \n    \ndf_commits = spark.read.parquet(os.path.join(gcs_folder, 'commits'))\nprint(f'Records read from dataframe *commits*: {df_commits.count():,.0f}')"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- commit: string (nullable = true)\n |-- tree: string (nullable = true)\n |-- parent: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- author: struct (nullable = true)\n |    |-- name: string (nullable = true)\n |    |-- email: string (nullable = true)\n |    |-- time_sec: long (nullable = true)\n |    |-- tz_offset: long (nullable = true)\n |    |-- date: struct (nullable = true)\n |    |    |-- seconds: long (nullable = true)\n |    |    |-- nanos: long (nullable = true)\n |-- committer: struct (nullable = true)\n |    |-- name: string (nullable = true)\n |    |-- email: string (nullable = true)\n |    |-- time_sec: long (nullable = true)\n |    |-- tz_offset: long (nullable = true)\n |    |-- date: struct (nullable = true)\n |    |    |-- seconds: long (nullable = true)\n |    |    |-- nanos: long (nullable = true)\n |-- subject: string (nullable = true)\n |-- message: string (nullable = true)\n |-- trailer: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- key: string (nullable = true)\n |    |    |-- value: string (nullable = true)\n |    |    |-- email: string (nullable = true)\n |-- difference: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- old_mode: long (nullable = true)\n |    |    |-- new_mode: long (nullable = true)\n |    |    |-- old_path: string (nullable = true)\n |    |    |-- new_path: string (nullable = true)\n |    |    |-- old_sha1: string (nullable = true)\n |    |    |-- new_sha1: string (nullable = true)\n |    |    |-- old_repo: string (nullable = true)\n |    |    |-- new_repo: string (nullable = true)\n |-- difference_truncated: boolean (nullable = true)\n |-- repo_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- encoding: string (nullable = true)\n\n"}], "source": "df_commits.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Contents\nUnique file contents of text files under 1 MiB on the HEAD branch.  \nCan be joined to `files` dataset using the id columns to identify the repository and file path."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 13:===================================================>(6980 + 1) / 6981]\r"}, {"name": "stdout", "output_type": "stream", "text": "Records read from dataframe *commits*: 281,191,977\nCPU times: user 2.08 s, sys: 477 ms, total: 2.56 s\nWall time: 8min 39s\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "%%time   \n    \ndf_contents = spark.read.parquet(os.path.join(gcs_folder, 'contents'))\nprint(f'Records read from dataframe *commits*: {df_contents.count():,.0f}')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- id: string (nullable = true)\n |-- size: long (nullable = true)\n |-- content: string (nullable = true)\n |-- binary: boolean (nullable = true)\n |-- copies: long (nullable = true)\n\n"}], "source": "df_contents.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Files\nFile metadata for all files at HEAD.  \nJoin with `contents` dataset on id columns to search text."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 17:===================================================>(1079 + 1) / 1080]\r"}, {"name": "stdout", "output_type": "stream", "text": "Records read from dataframe *files*: 2,309,424,945\nCPU times: user 341 ms, sys: 72.5 ms, total: 413 ms\nWall time: 1min 26s\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "%%time   \n    \ndf_files = spark.read.parquet(os.path.join(gcs_folder, 'files'))\nprint(f'Records read from dataframe *files*: {df_files.count():,.0f}')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- repo_name: string (nullable = true)\n |-- ref: string (nullable = true)\n |-- path: string (nullable = true)\n |-- mode: long (nullable = true)\n |-- id: string (nullable = true)\n |-- symlink_target: string (nullable = true)\n\n"}], "source": "df_files.printSchema()"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": "# Get a cleaned commits dataframe by selecting variables that are needed.\n# Sort time after 2008 and before today\ncommits_sample = df_commits.filter((col(\"author.time_sec\") > 1199167201) & (col(\"author.time_sec\") < 1733594559)).sample(withReplacement=False, fraction=0.01, seed=42)\ncleaned_c = commits_sample.select(col(\"commit\"),\n                                  col(\"author.name\").alias(\"author_name\"),\n                                  col(\"author.time_sec\").alias(\"timestamp\"),\n                                  col(\"subject\"),\n                                  col(\"message\"),\n                                  explode(col(\"repo_name\")).alias(\"repo_name\"))\ncleaned_c = cleaned_c.withColumn('date', from_unixtime('timestamp').cast(DateType()))"}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 20:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----------+\n|              commit|         author_name| timestamp|             subject|             message|           repo_name|      date|\n+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----------+\n|be41ab16c41b28840...|conda-forge-coord...|1596039929|Updated the yacro...|Updated the yacro...|conda-forge/feeds...|2020-07-29|\n|7c87590a3c12094e1...|\u042f\u0448\u0438\u043d\u0430 \u0410\u043b\u0435\u043a\u0441\u0430\u043d\u0434\u0440\u0430 ...|1408946175|[NA]: \u043f\u043e \u043f\u0438\u0441\u044c\u043c\u043e \u043e...|[NA]: \u043f\u043e \u043f\u0438\u0441\u044c\u043c\u043e \u043e...|2gis/nuclear-aggr...|2014-08-25|\n|68e5e9e74b0d619e2...|                ilor|1237148567|        rm empty dir|rm empty dir\n\ngit...|battle-for-wesnot...|2009-03-15|\n|e4723fd447eb44da5...|            echristo|1394064053|constify a few ac...|constify a few ac...|tarunprabhu/Drago...|2014-03-06|\n|005a597f5da0a69e6...|NetKAN inflator R...|1633019394|NetKAN added mod ...|NetKAN added mod ...|  Olympic1/CKAN-meta|2021-09-30|\n+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "cleaned_c.show(5)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import datetime\nimport pytz\n\ndatetime.datetime.now(pytz.timezone('US/Central')).strftime(\"%a, %d %B %Y %H:%M:%S\")"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [], "source": "df_lan_license = df_languages.select(\"repo_name\", \"language.name\").join(df_licenses, \"repo_name\")"}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 22:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "Records read from dataframe *languages*: 3,325,634\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "print(f'Records read from dataframe *languages*: {df_lan_license.count():,.0f}')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df_lan_license.write.parquet('gs://ads-sjhuang-final/notebooks/sjhuang/lan_license01',mode=\"overwrite\")"}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [], "source": "sample_1 = cleaned_c.join(df_lan_license, on = 'repo_name', how = 'left')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/html": "<table border='1'>\n<tr><th>repo_name</th><th>commit</th><th>author_name</th><th>timestamp</th><th>subject</th><th>message</th><th>date</th><th>name</th><th>license</th></tr>\n<tr><td>001szymon/uBlock</td><td>fb62bbc29e437a3e5...</td><td>gorhill</td><td>1404316949</td><td>this fixes #12, #37</td><td>this fixes #12, #37\n</td><td>2014-07-02</td><td>[CSS, HTML, JavaS...</td><td>gpl-3.0</td></tr>\n<tr><td>001szymon/uBlock</td><td>60295031dfba85299...</td><td>gorhill</td><td>1404428298</td><td>making videos ava...</td><td>making videos ava...</td><td>2014-07-03</td><td>[CSS, HTML, JavaS...</td><td>gpl-3.0</td></tr>\n<tr><td>001szymon/uBlock</td><td>a5725cb6073b6f4ab...</td><td>gorhill</td><td>1425424377</td><td>for network reque...</td><td>for network reque...</td><td>2015-03-03</td><td>[CSS, HTML, JavaS...</td><td>gpl-3.0</td></tr>\n<tr><td>001szymon/uBlock</td><td>62c8ffbcc4e75073a...</td><td>AlexVallat</td><td>1426012456</td><td>Merge branch &#x27;mas...</td><td>Merge branch &#x27;mas...</td><td>2015-03-10</td><td>[CSS, HTML, JavaS...</td><td>gpl-3.0</td></tr>\n<tr><td>001szymon/uBlock</td><td>8a19f32373e355022...</td><td>Deathamns</td><td>1419021597</td><td>Remove duplicate ...</td><td>Remove duplicate ...</td><td>2014-12-19</td><td>[CSS, HTML, JavaS...</td><td>gpl-3.0</td></tr>\n</table>\n", "text/plain": "+----------------+--------------------+-----------+----------+--------------------+--------------------+----------+--------------------+-------+\n|       repo_name|              commit|author_name| timestamp|             subject|             message|      date|                name|license|\n+----------------+--------------------+-----------+----------+--------------------+--------------------+----------+--------------------+-------+\n|001szymon/uBlock|fb62bbc29e437a3e5...|    gorhill|1404316949| this fixes #12, #37|this fixes #12, #37\n|2014-07-02|[CSS, HTML, JavaS...|gpl-3.0|\n|001szymon/uBlock|60295031dfba85299...|    gorhill|1404428298|making videos ava...|making videos ava...|2014-07-03|[CSS, HTML, JavaS...|gpl-3.0|\n|001szymon/uBlock|a5725cb6073b6f4ab...|    gorhill|1425424377|for network reque...|for network reque...|2015-03-03|[CSS, HTML, JavaS...|gpl-3.0|\n|001szymon/uBlock|62c8ffbcc4e75073a...| AlexVallat|1426012456|Merge branch 'mas...|Merge branch 'mas...|2015-03-10|[CSS, HTML, JavaS...|gpl-3.0|\n|001szymon/uBlock|8a19f32373e355022...|  Deathamns|1419021597|Remove duplicate ...|Remove duplicate ...|2014-12-19|[CSS, HTML, JavaS...|gpl-3.0|\n+----------------+--------------------+-----------+----------+--------------------+--------------------+----------+--------------------+-------+"}, "execution_count": 24, "metadata": {}, "output_type": "execute_result"}], "source": "sample_1.limit(5)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "sample_1.write.parquet('gs://ads-sjhuang-final/notebooks/sjhuang/sample01',mode=\"overwrite\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 4}